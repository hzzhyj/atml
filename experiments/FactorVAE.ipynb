{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0zE2pH0MQs27"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path+\"/models\")\n",
    "sys.path.append(module_path+\"/train\")\n",
    "sys.path.append(module_path+\"/experiments\")\n",
    "sys.path.append(module_path+\"/datasets\")\n",
    "\n",
    "from factor_vae import FactorVAEDSprites, Discriminator, FactorVAEcnn\n",
    "from beta_vae import Classifier, BetaVAECelebA\n",
    "from datasets import train_test_random_split, load_dsprites, CustomDSpritesDatasetFactorVAE, AddUniformNoise, CustomDSpritesDataset, AddGeneratedNoise\n",
    "from train import train_factor_vae, test_factor_vae, train_beta_vae, test_beta_vae\n",
    "from entanglement_metric import entanglement_metric_factor_vae, entanglement_metric_beta_vae\n",
    "from utils import save_checkpoint_factorvae, load_checkpoint_factorvae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SOqvu4-6KCX3",
    "outputId": "f706b5ce-cb56-4170-8fba-aec4f68d7fad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kTA2KizqtEEJ"
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sGs_WN8y4NrA"
   },
   "outputs": [],
   "source": [
    "dataset = load_dsprites(\"../datasets/dsprites.npz\",False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xbLpHrdNrU64"
   },
   "outputs": [],
   "source": [
    "#transform = None\n",
    "transform = AddGeneratedNoise(\"atml/datasets/noisenet.pth\",device)\n",
    "#transform = transforms.Compose([AddUniformNoise(-.1, .1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b4Ab4lsC4-MG"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "#data_ = CustomDSpritesDataset(dataset,seed=seed,transform=transform)\n",
    "data_ = CustomDSpritesDatasetFactorVAE(dataset,seed=seed,transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4_iXbN-Y7s_2"
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_random_split(data_, 0.8, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XbvdDC2A7vG-"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aVX5BIvdrNJC"
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss_lists, title):\n",
    "    if not isinstance(loss_lists[0],list):\n",
    "        loss_lists=[loss_lists]\n",
    "    for loss_list in loss_lists:\n",
    "        plt.plot(np.arange(1,len(loss_list)+1, 1), loss_list)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training \"+title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "arsWn4kcUapc"
   },
   "outputs": [],
   "source": [
    "lr1s = [0.01]\n",
    "params = [[5,0.0001]]\n",
    "step = 50\n",
    "starts= np.arange(0,50,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOn2XqPDUtuK",
    "outputId": "c7892141-0f58-4793-a9b8-d49c358fbf80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr1 : 0.01\n",
      "lr2 : 0.0001\n",
      "gamma : 5\n",
      "epochs : 0 to 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished, loss: 60.33667491707537, recon loss: 46.344402058463956, kl div: 13.774261785457181, TC loss: 0.043602170549187726, discriminator loss: 0.682157525081291\n",
      "Epoch 1 finished, loss: 39.48241013288498, recon loss: 31.04262187745836, kl div: 8.300892201976644, TC loss: 0.027779216067983725, discriminator loss: 0.6853030270883917\n",
      "Epoch 2 finished, loss: 38.87419954977102, recon loss: 30.42804006776876, kl div: 8.326883636777186, TC loss: 0.023855174140029096, discriminator loss: 0.6863800497635061\n",
      "Epoch 3 finished, loss: 38.577981761760185, recon loss: 30.129345694142913, kl div: 8.341780984774232, TC loss: 0.021371015859800637, discriminator loss: 0.687119894706282\n",
      "Epoch 4 finished, loss: 38.37926313446628, recon loss: 29.93422055741151, kl div: 8.347666165584492, TC loss: 0.01947528271326367, discriminator loss: 0.6875606259175887\n",
      "Epoch 5 finished, loss: 38.25119371008542, recon loss: 29.80379956588149, kl div: 8.350886991040575, TC loss: 0.019301434449011542, discriminator loss: 0.6876656500842526\n"
     ]
    }
   ],
   "source": [
    "for lr1 in lr1s:\n",
    "    for (gamma,lr2) in params:\n",
    "        for start in starts:\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            np.random.seed(seed)\n",
    "            print(\"lr1 :\", lr1)\n",
    "            print(\"lr2 :\", lr2)\n",
    "            print(\"gamma :\", gamma)\n",
    "            print(\"epochs :\", str(start)+\" to \"+str(start+step))\n",
    "            model = FactorVAEDSprites()\n",
    "            model.to(device)\n",
    "            discriminator = Discriminator(nb_layers=6,hidden_dim=1000)\n",
    "            discriminator.to(device)\n",
    "            vae_optimizer = torch.optim.Adagrad(model.parameters(),lr=lr1)\n",
    "            discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr2)\n",
    "            #old_name = \"/content/drive/My Drive/atml_models/factorvae_epochs\"+str(start)+\"_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".pth.tar\"\n",
    "            #model,discriminator,vae_optimizer,discriminator_optimizer,start_epoch = load_checkpoint_factorvae(model,discriminator,vae_optimizer,discriminator_optimizer,old_name)\n",
    "            train_losses_list, recon_losses_list, kl_divs_list, tc_losses_list, discriminator_losses_list = train_factor_vae(model, discriminator, step, train_loader, vae_optimizer, discriminator_optimizer, gamma, 'gaussian', device = device)\n",
    "            torch.save(model,\"/content/drive/My Drive/atml_models/noisy3_factorvae_epochs50_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".dat\")\n",
    "            np.save(\"/content/drive/My Drive/atml_models/noisy3_recon_loss_factorvae_epochs50_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".npy\",recon_losses_list)\n",
    "            np.save(\"/content/drive/My Drive/atml_models/noisy3_kl_divs_factorvae_epochs50_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".npy\",kl_divs_list)\n",
    "            np.save(\"/content/drive/My Drive/atml_models/noisy3_tc_loss_factorvae_epochs50_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".npy\",tc_losses_list)\n",
    "            plot_loss(train_losses_list, \"Total loss\")\n",
    "            plot_loss(recon_losses_list, \"Reconstruction loss\")\n",
    "            plot_loss(kl_divs_list, \"KL divergence\")\n",
    "            plot_loss(tc_losses_list, \"TC loss\")\n",
    "            plot_loss(discriminator_losses_list, \"Discriminator loss\")\n",
    "            #new_name = \"/content/drive/My Drive/atml_models/factorvae_epochs\"+str(start+step)+\"_gamma\"+str(gamma)+\"_lrvae\"+str(lr1)+\"_lrd\"+str(lr2)+\".pth.tar\"\n",
    "            #save_checkpoint_factorvae(model, discriminator, vae_optimizer, discriminator_optimizer, new_name, start+step)\n",
    "            test_factor_vae(model, discriminator, test_loader, gamma, 'gaussian', device=device)\n",
    "            print(\"Factor Vae metric: \")\n",
    "            losses = entanglement_metric_factor_vae(model, data_, 500, 500, random_seeds=5, device=device,seed=seed)\n",
    "            print(\"Accuracy: \"+str(np.mean(losses)))\n",
    "            print(\"Beta Vae metric: \")\n",
    "            classifier = Classifier()\n",
    "            classifier.to(device)\n",
    "            optimizer = torch.optim.Adagrad(classifier.parameters(), lr=1e-2)\n",
    "            losses, accuracies, test_accuracies = entanglement_metric_beta_vae(model, classifier, optimizer, 2000,  data_, 1000, 50, random_seeds=2, device=device,seed=seed)\n",
    "            print(\"Accuracy: \"+str(np.mean(test_accuracies)))\n",
    "            plot_loss(losses, \"NLL Loss\")\n",
    "            plot_loss(accuracies, \"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKe3bhq-PEek"
   },
   "source": [
    "gamma 5 lr2 0.0001 ou 0.00001\n",
    "\n",
    "gamma 40 lr2 0.00005"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "atml.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
